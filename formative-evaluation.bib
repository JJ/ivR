@inproceedings{leslie2020specifications,
  title={Specifications grading: What it is, and lessons learned},
  author={Leslie, Paula and Lundblom, Erin},
  booktitle={Seminars in Speech and Language},
  volume={41},
  number={04},
  pages={298--309},
  year={2020},
  organization={Thieme Medical Publishers}
}
@book{nilson2015specifications,
  title={Specifications grading: Restoring rigor, motivating students, and saving faculty time},
  author={Nilson, Linda B and Stanny, Claudia J},
  year={2015},
  publisher={Routledge}
}
@inproceedings{sampangi2024programming,
  title={Programming Assignment Ungrading as a License to Learn: Implementing Specifications Grading in the Undergraduate Web Development Classroom},
  author={Sampangi, Raghav V and Poitras, Eric and Barrera Machuca, Mayra D},
  booktitle={Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
  pages={1808--1809},
  year={2024}
}

@article{Marin2022Student,
	author = {Mar{\' i}n, Victoria I.},
	journal = {Studies in Technology Enhanced Learning},
	number = {2},
	year = {2022},
	month = {mar 7},
	note = {https://stel.pubpub.org/pub/02-02-marin-2021},
	publisher = {},
	title = {Student-centred learning in higher education in times of {Covid}-19: A critical analysis},
	volume = {2},
}

@inproceedings{10.1145/3626252.3630953,
author = {Decker, Adrienne and Edwards, Stephen H. and McSkimming, Brian M. and Edmison, Bob and Rorrer, Audrey and P\'{e}rez Qui\~{n}ones, Manuel A.},
title = {Transforming Grading Practices in the Computing Education Community},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630953},
doi = {10.1145/3626252.3630953},
abstract = {It is often the case that computer science classrooms use traditional grading practices where points are allocated to assignments, mistakes result in point deductions, and assignment scores are combined using some form of weighted averaging to determine grades. Unfortunately, traditional grading practices have been shown to reduce achievement, discourage students, and suppress effort to such an extent that some common elements of traditional grading practices have been termed toxic. Using grades to reward or punish student behavior does not encourage learning and instead increases anxiety and stress. These toxic elements are present throughout computing education and computer science classrooms in the form of late penalties, lack of credit for code that doesn't compile or pass certain unit tests, among others. These types of metrics, that evaluate behavior are often influenced by implicit bias, factors outside of the classrooms (e.g., part-time employment), and family life situations (e.g., students who are caregivers). Often, students in these situations are disproportionately from low-socioeconomic backgrounds and predominantly students of color. Through this paper, we will present a case for adoption of equitable grading practices and a call for additional support in classroom and teaching technologies as well as support from administrations both at the department and university level. By adopting a community of practice approach, we argue that we can support new faculty making these changes, which would be more equitable and inclusive. Further, these practices have been shown to better support student learning and can help increase student learning gains and retention.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {276–282},
numpages = {7},
keywords = {equitable grading, grading for equity, grading practices},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3626252.3630929,
author = {Harrington, Brian and Galal, Abdalaziz and Nalluri, Rohita and Nasiha, Faiza and Vadarevu, Anagha},
title = {Specifications and Contract Grading in Computer Science Education},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630929},
doi = {10.1145/3626252.3630929},
abstract = {With the recent growth in popularity of alternative evaluation methods, two methodologies have become particularly prevalent in CS education literature: Specifications grading and contract grading. Recent work has shown that these novel evaluation approaches can have positive impacts in the classroom and lead to more equitable outcomes for students. However, there is not yet a consensus on terminology, implementation details and best practices. In this work, we review the literature on the use of specifications and contract grading in CS education. We find that while there is a good deal of promising research, there is a great deal of variation in methodologies and a sparsity of evaluation of the efficacy of learning outcomes.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {477–483},
numpages = {7},
keywords = {alternative evaluation, contract grading, literature review, mastery grading, specifications grading},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3626252.3630929,
author = {Harrington, Brian and Galal, Abdalaziz and Nalluri, Rohita and Nasiha, Faiza and Vadarevu, Anagha},
title = {Specifications and Contract Grading in Computer Science Education},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630929},
doi = {10.1145/3626252.3630929},
abstract = {With the recent growth in popularity of alternative evaluation methods, two methodologies have become particularly prevalent in CS education literature: Specifications grading and contract grading. Recent work has shown that these novel evaluation approaches can have positive impacts in the classroom and lead to more equitable outcomes for students. However, there is not yet a consensus on terminology, implementation details and best practices. In this work, we review the literature on the use of specifications and contract grading in CS education. We find that while there is a good deal of promising research, there is a great deal of variation in methodologies and a sparsity of evaluation of the efficacy of learning outcomes.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {477–483},
numpages = {7},
keywords = {alternative evaluation, contract grading, literature review, mastery grading, specifications grading},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

@article{tessmer1994formative,
  title={Formative evaluation alternatives},
  author={Tessmer, Martin},
  journal={Performance Improvement Quarterly},
  volume={7},
  number={1},
  pages={3--18},
  year={1994},
  publisher={Wiley Online Library}
}

@article{doi:10.1177/001440298605300301,
author = {Lynn S. Fuchs and Douglas Fuchs},
title ={Effects of Systematic Formative Evaluation: A Meta-Analysis},

journal = {Exceptional Children},
volume = {53},
number = {3},
pages = {199-208},
year = {1986},
doi = {10.1177/001440298605300301},
    note ={PMID: 3792417},
URL = {https://doi.org/10.1177/001440298605300301},
eprint = {https://doi.org/10.1177/001440298605300301},
    abstract = { This meta-analysis investigated the effects of formative evaluation procedures on student achievement. The data source was 21 controlled studies, which generated 96 relevant effect sizes, with an average weighted effect size of .70. The magnitude of the effect of formative evaluation was associated with publication type, data-evaluation method, data display, and use of behavior modification. Implications for special education practice are discussed. }
}

@article{scriven1996theory,
  title={The theory behind practical evaluation},
  author={Scriven, Michael},
  journal={Evaluation},
  volume={2},
  number={4},
  pages={393--404},
  year={1996},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@InBook{scriven1967social,
  chapter={THE METHODOLOGY OF EVALUATION},
  author={Scriven, Michael},
  title={Perspectives in curriculum evaluation},
  editors={Tyler, Gagné, Scriven},
  year={1967},
  publisher={Rand McNally}
}

 @article{article_1272054, title={Learning analytics in formative assessment: A systematic literature review}, journal={Journal of Measurement and Evaluation in Education and Psychology}, volume={14}, pages={359–381}, year={2023}, DOI={10.21031/epod.1272054}, author={ZHANG, Ke and YILMAZ, Ramazan and USTUN, Ahmet Berk and KARAOĞLAN YILMAZ, Fatma Gizem}, keywords={Learning analytics, formative assessment, assessment analytics, bibliometrics}, abstract={This systematic review examines the use of learning analytics (LA) in formative assessment (FA). LA is a powerful tool that can support FA by providing real-time feedback to students and teachers. The review analyzes studies published on Web of Science and Scopus databases between 2011 and 2022 that provide an overview of the current state of published research on the use of LA for FA in diverse learning environments and through different delivery modes. This review also explores the significant potential of LA in FA practices in digital learning. A total of 63 studies met all selection criteria and were fully reviewed by conducting multiple analyses including selected bibliometrics, a categorical meta-trends analysis and inductive content analysis. The results indicate that the number of LA in FA studies has experienced a significant surge over the past decade. The results also show the current state of research on LA in FA, through a range of disciplines, journals, research methods, learning environments and delivery modes. This review can help inform the implementation of LA in educational contexts to support effective FA practices. However, the review also highlights the need for further research.}, number={Özel Sayı}, publisher={Eğitimde ve Psikolojide Ölçme ve Değerlendirme Derneği} }

@article{https://doi.org/10.1111/bjet.13276,
author = {Bulut, Okan and Gorgun, Guher and Yildirim-Erbasli, Seyma N. and Wongvorachan, Tarid and Daniels, Lia M. and Gao, Yizhu and Lai, Ka Wing and Shin, Jinnie},
title = {Standing on the shoulders of giants: Online formative assessments as the foundation for predictive learning analytics models},
journal = {British Journal of Educational Technology},
volume = {54},
number = {1},
pages = {19-39},
keywords = {formative assessment, learning analytics, learning management system, log data, predictive modelling},
doi = {https://doi.org/10.1111/bjet.13276},
url = {https://bera-journals.onlinelibrary.wiley.com/doi/abs/10.1111/bjet.13276},
eprint = {https://bera-journals.onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.13276},
abstract = {Abstract As universities around the world have begun to use learning management systems (LMSs), more learning data have become available to gain deeper insights into students' learning processes and make data-driven decisions to improve student learning. With the availability of rich data extracted from the LMS, researchers have turned much of their attention to learning analytics (LA) applications using educational data mining techniques. Numerous LA models have been proposed to predict student achievement in university courses. To design predictive LA models, researchers often follow a data-driven approach that prioritizes prediction accuracy while sacrificing theoretical links to learning theory and its pedagogical implications. In this study, we argue that instead of complex variables (e.g., event logs, clickstream data, timestamps of learning activities), data extracted from online formative assessments should be the starting point for building predictive LA models. Using the LMS data from multiple offerings of an asynchronous undergraduate course, we analysed the utility of online formative assessments in predicting students' final course performance. Our findings showed that the features extracted from online formative assessments (e.g., completion, timestamps and scores) served as strong and significant predictors of students' final course performance. Scores from online formative assessments were consistently the strongest predictor of student performance across the three sections of the course. The number of clicks in the LMS and the time difference between first access and due dates of formative assessments were also significant predictors. Overall, our findings emphasize the need for online formative assessments to build predictive LA models informed by theory and learning design. Practitioner notes What is already known about this topic Higher education institutions often use learning analytics for the early identification of low-performing students or students at risk of dropping out. Most predictive models in learning analytics rely on immutable student characteristics (e.g., gender, race and socioeconomic status) and complex variables extracted from log data within a learning management system. Prioritizing prediction accuracy without theory orientation often yields “black-box” models that fail to inform educators on what remedies need to be taken to improve student learning. What this paper adds Predictive models in learning analytics should consider learning theory, pedagogy and learning design to identify key predictors of student learning. Online formative assessments can be a starting point for building predictive models that are not only accurate but also provide educators with actionable insights on how student learning can be improved. Time-related and score-related features extracted from online formative assessments are particularly useful for predicting students' course performance. Implications for practice and/or policy This study provides strong evidence for using online formative assessments as the foundation for predictive models in learning analytics. Student data from online formative assessments can help educators provide students with feedback while informing future formative assessment cycles. Higher education institutions should avoid the hype around complex data from learning management systems and instead rely on effective learning tools such as online formative assessments to revolutionize the use of learning analytics.},
year = {2023}
}

@article{TEMPELAAR2018408,
title = {Student profiling in a dispositional learning analytics application using formative assessment},
journal = {Computers in Human Behavior},
volume = {78},
pages = {408-420},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217304776},
author = {Dirk Tempelaar and Bart Rienties and Jenna Mittelmeier and Quan Nguyen},
keywords = {Learning analytics, Formative assessment, Learning dispositions, Dispositional learning analytics, e-tutorial},
abstract = {How learning disposition data can help us translating learning feedback from a learning analytics application into actionable learning interventions, is the main focus of this empirical study. It extends previous work (Tempelaar, Rienties, & Giesbers, 2015), where the focus was on deriving timely prediction models in a data rich context, encompassing trace data from learning management systems, formative assessment data, e-tutorial trace data as well as learning dispositions. In this same educational context, the current study investigates how the application of cluster analysis based on e-tutorial trace data allows student profiling into different at-risk groups, and how these at-risk groups can be characterized with the help of learning disposition data. It is our conjecture that establishing a chain of antecedent-consequence relationships starting from learning disposition, through student activity in e-tutorials and formative assessment performance, to course performance, adds a crucial dimension to current learning analytics studies: that of profiling students with descriptors that easily lend themselves to the design of educational interventions.}
}


@article{fuchs1986effects,
  title={Effects of systematic formative evaluation: A meta-analysis},
  author={Fuchs, Lynn S and Fuchs, Douglas},
  journal={Exceptional children},
  volume={53},
  number={3},
  pages={199--208},
  year={1986},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
