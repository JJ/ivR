@article{tessmer1994formative,
  title={Formative evaluation alternatives},
  author={Tessmer, Martin},
  journal={Performance Improvement Quarterly},
  volume={7},
  number={1},
  pages={3--18},
  year={1994},
  publisher={Wiley Online Library}
}

@article{doi:10.1177/001440298605300301,
author = {Lynn S. Fuchs and Douglas Fuchs},
title ={Effects of Systematic Formative Evaluation: A Meta-Analysis},

journal = {Exceptional Children},
volume = {53},
number = {3},
pages = {199-208},
year = {1986},
doi = {10.1177/001440298605300301},
    note ={PMID: 3792417},

URL = { 
    
        https://doi.org/10.1177/001440298605300301
    
    

},
eprint = { 
    
        https://doi.org/10.1177/001440298605300301
    
    

}
,
    abstract = { This meta-analysis investigated the effects of formative evaluation procedures on student achievement. The data source was 21 controlled studies, which generated 96 relevant effect sizes, with an average weighted effect size of .70. The magnitude of the effect of formative evaluation was associated with publication type, data-evaluation method, data display, and use of behavior modification. Implications for special education practice are discussed. }
}

@article{otero2018flipped,
  title={Flipped learning and formative evaluation in higher education},
  author={Otero-Saborido, Fernando M and S{\'a}nchez-Oliver, Antonio J and Grimaldi-Puyana, Mois{\'e}s and {\'A}lvarez-Garc{\'\i}a, Jos{\'e}},
  journal={Education+ Training},
  volume={60},
  number={5},
  pages={421--430},
  year={2018},
  publisher={Emerald Publishing Limited}
}
@article{scriven1996theory,
  title={The theory behind practical evaluation},
  author={Scriven, Michael},
  journal={Evaluation},
  volume={2},
  number={4},
  pages={393--404},
  year={1996},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@InBook{scriven1967social,
  chapter={THE METHODOLOGY OF EVALUATION},
  author={Scriven, Michael},
  title={Perspectives in curriculum evaluation},
  editors={Tyler, Gagné, Scriven}
  year={1967},
  publisher={Rand McNally}
}

 @article{article_1272054, title={Learning analytics in formative assessment: A systematic literature review}, journal={Journal of Measurement and Evaluation in Education and Psychology}, volume={14}, pages={359–381}, year={2023}, DOI={10.21031/epod.1272054}, author={ZHANG, Ke and YILMAZ, Ramazan and USTUN, Ahmet Berk and KARAOĞLAN YILMAZ, Fatma Gizem}, keywords={Learning analytics, formative assessment, assessment analytics, bibliometrics}, abstract={This systematic review examines the use of learning analytics (LA) in formative assessment (FA). LA is a powerful tool that can support FA by providing real-time feedback to students and teachers. The review analyzes studies published on Web of Science and Scopus databases between 2011 and 2022 that provide an overview of the current state of published research on the use of LA for FA in diverse learning environments and through different delivery modes. This review also explores the significant potential of LA in FA practices in digital learning. A total of 63 studies met all selection criteria and were fully reviewed by conducting multiple analyses including selected bibliometrics, a categorical meta-trends analysis and inductive content analysis. The results indicate that the number of LA in FA studies has experienced a significant surge over the past decade. The results also show the current state of research on LA in FA, through a range of disciplines, journals, research methods, learning environments and delivery modes. This review can help inform the implementation of LA in educational contexts to support effective FA practices. However, the review also highlights the need for further research.}, number={Özel Sayı}, publisher={Eğitimde ve Psikolojide Ölçme ve Değerlendirme Derneği} }

@article{https://doi.org/10.1111/bjet.13276,
author = {Bulut, Okan and Gorgun, Guher and Yildirim-Erbasli, Seyma N. and Wongvorachan, Tarid and Daniels, Lia M. and Gao, Yizhu and Lai, Ka Wing and Shin, Jinnie},
title = {Standing on the shoulders of giants: Online formative assessments as the foundation for predictive learning analytics models},
journal = {British Journal of Educational Technology},
volume = {54},
number = {1},
pages = {19-39},
keywords = {formative assessment, learning analytics, learning management system, log data, predictive modelling},
doi = {https://doi.org/10.1111/bjet.13276},
url = {https://bera-journals.onlinelibrary.wiley.com/doi/abs/10.1111/bjet.13276},
eprint = {https://bera-journals.onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.13276},
abstract = {Abstract As universities around the world have begun to use learning management systems (LMSs), more learning data have become available to gain deeper insights into students' learning processes and make data-driven decisions to improve student learning. With the availability of rich data extracted from the LMS, researchers have turned much of their attention to learning analytics (LA) applications using educational data mining techniques. Numerous LA models have been proposed to predict student achievement in university courses. To design predictive LA models, researchers often follow a data-driven approach that prioritizes prediction accuracy while sacrificing theoretical links to learning theory and its pedagogical implications. In this study, we argue that instead of complex variables (e.g., event logs, clickstream data, timestamps of learning activities), data extracted from online formative assessments should be the starting point for building predictive LA models. Using the LMS data from multiple offerings of an asynchronous undergraduate course, we analysed the utility of online formative assessments in predicting students' final course performance. Our findings showed that the features extracted from online formative assessments (e.g., completion, timestamps and scores) served as strong and significant predictors of students' final course performance. Scores from online formative assessments were consistently the strongest predictor of student performance across the three sections of the course. The number of clicks in the LMS and the time difference between first access and due dates of formative assessments were also significant predictors. Overall, our findings emphasize the need for online formative assessments to build predictive LA models informed by theory and learning design. Practitioner notes What is already known about this topic Higher education institutions often use learning analytics for the early identification of low-performing students or students at risk of dropping out. Most predictive models in learning analytics rely on immutable student characteristics (e.g., gender, race and socioeconomic status) and complex variables extracted from log data within a learning management system. Prioritizing prediction accuracy without theory orientation often yields “black-box” models that fail to inform educators on what remedies need to be taken to improve student learning. What this paper adds Predictive models in learning analytics should consider learning theory, pedagogy and learning design to identify key predictors of student learning. Online formative assessments can be a starting point for building predictive models that are not only accurate but also provide educators with actionable insights on how student learning can be improved. Time-related and score-related features extracted from online formative assessments are particularly useful for predicting students' course performance. Implications for practice and/or policy This study provides strong evidence for using online formative assessments as the foundation for predictive models in learning analytics. Student data from online formative assessments can help educators provide students with feedback while informing future formative assessment cycles. Higher education institutions should avoid the hype around complex data from learning management systems and instead rely on effective learning tools such as online formative assessments to revolutionize the use of learning analytics.},
year = {2023}
}

@article{TEMPELAAR2018408,
title = {Student profiling in a dispositional learning analytics application using formative assessment},
journal = {Computers in Human Behavior},
volume = {78},
pages = {408-420},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217304776},
author = {Dirk Tempelaar and Bart Rienties and Jenna Mittelmeier and Quan Nguyen},
keywords = {Learning analytics, Formative assessment, Learning dispositions, Dispositional learning analytics, e-tutorial},
abstract = {How learning disposition data can help us translating learning feedback from a learning analytics application into actionable learning interventions, is the main focus of this empirical study. It extends previous work (Tempelaar, Rienties, & Giesbers, 2015), where the focus was on deriving timely prediction models in a data rich context, encompassing trace data from learning management systems, formative assessment data, e-tutorial trace data as well as learning dispositions. In this same educational context, the current study investigates how the application of cluster analysis based on e-tutorial trace data allows student profiling into different at-risk groups, and how these at-risk groups can be characterized with the help of learning disposition data. It is our conjecture that establishing a chain of antecedent-consequence relationships starting from learning disposition, through student activity in e-tutorials and formative assessment performance, to course performance, adds a crucial dimension to current learning analytics studies: that of profiling students with descriptors that easily lend themselves to the design of educational interventions.}
}