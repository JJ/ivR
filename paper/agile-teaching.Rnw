%%
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{XXXXXXX.XXXXXXX}

\acmConference[ITICSE'24]{ITICSE'24}{June 03--05,  2018}{Milan, IT}
\acmISBN{978-1-4503-XXXX-X/18/06}

\begin{document}

\title{Evaluative formation for agile teaching}

\author{JJ Merelo-Guerv√≥s}
\email{jmerelo@ugr.es}
\orcid{0000-0002-1385-9741}
\affiliation{%
  \institution{Department of Computer Engineering, Automatics and Robotics and CITIC, University of Granada}
  \city{Granada}
  \country{Spain}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{JJ Merelo}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011074.10011081.10011082.10011083</concept_id>
       <concept_desc>Software and its engineering~Agile software development</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Agile software development}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Software engineering, agile methodologies}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Teaching software engineering represents not only a challenge, but a series of challenges to the teacher, who needs the student to acquire a series of marketable skills at the same time that this acquisition is assessed in order to obtain a grade. At the same time, mainstream software development follows an {\em agile mindset}, following the Agile Manifesto \cite{agilemanifesto}, which is a customer-centered philosophy that maintains that any change in code should add value to the customer, either directly or indirectly. Teaching mindsets of philosophies is a notch more complicated than teaching skills; evaluating mindsets is even more complex. But that is what an industry with its ever growing {\em need for talent} is asking for.

One of the ways of teaching a mindset is having that same mindset while teaching; that is, if you want to teach agile, be agile while teaching \cite{chun2004agile,krehbiel2017agile}. One step into this mindset is simply to consider class experience a product for which students, collectively and individually, are the customers, and use industry-standard tools such as GitHub to produce class materials, create course-oriented milestones, as well as generate issues with the problems that need to be solved, content-wise or related to any software that you might have developed to support teaching.

You can, however, go a bit further and embrace agile in all stages of the teaching/learnign process. Several researchers have proposed manifestos in agile teaching, so the first step would be to understand those manifestos and adopt them where possible. In our case, the manifesto created by Krehbiel et al. \cite{krehbiel2017agile} seems the closest to what we have working with; this manifesto emphasizes adaptability to personal (and other) circumstances, teamwork, focusing on learning objectives and not on grades, autonomous learning by the student, practical working instead of theoretical teaching, and, over all, continuous improvement.

Manifestos are never enough either in development or teaching, and agile development encourages the adoption of best practices in every phase of development; but the last part of the manifesto which talks about continuous improvement, as well as the fact that we are talking about {\em best} practices, would need some way of measuring progress and how adopted practices affect the classroom. This is why we would like to add a new item to the manifesto: {\em learning analytics} \cite{clow2013overview} over anecdotal evidence or end-of-class failure/success rates.

Within this agile mindset, there are teaching methodologies that fit better; adopting these best practices is the first step towards agile teaching. We will focus on two of them that fit well with each other: project-based learning gives the student autonomy (which is one of the items in the manifesto) through the development of a project across one or several subjects \cite{krajcik2006project}; formative evaluation \cite{tessmer1994formative} also fits the autonomous learning part of the manifesto, but also focuses on learning objectives, since it pursues helping the student through continuous feedback on their submitted work (that advances the project mentioned above). This feedback helps continuous improvement of the project they are working with, which is other of the items in the manifesto.

In this paper we will describe the experience carried out in the last three school years in a 4th year (7th semester) subject in the Computer Science degree at the University of Granada, Spain; the subject deals with Cloud Computing, so the project needs to be working towards that final objective. But alongside the description of the experience, we will try to answer the following research questions:\begin{itemize}
\item {\bf RQ1:} Can you leverage metrics to improve individual and collective learning?
\item {\bf RQ2:} Are there early indicators or class success or failure?
\item {\bf RQ3:} Can we measure the effect of flipped learning through the gathered software analytics?
\end{itemize}

The rest of the paper is organized as follows: next we will present the state of the art in learning analytics applied to evaluative formation and flipped learning. The class setup, as well as the way data is collected will be presented next, in Section \ref{sec:setup}. The results of the analysis will be presented in Section \ref{sec:results} along with the answers to the research questions, and finally we will draw some conclusions in Section \ref{sec:conclusions}.

\section{State of the art}

Learning analytics (LA) \cite{clow2013overview} have been applied to all kind of learning processes and subjects; this includes problem-based learning combined with formative assessment (FA); \cite{aljohani2013learning} mentions that learning analytics and formative assessment work well together since they both provide immediate feedback on the quality of the learning process; however, it can be argued that LA is an essential part of FA, since a deep understanding of the objectives achieved by students is needed in order to provide student-specific feedback. In general, however, LA will refer to additional, macro-measurements at a class level or analytics that compare student performance with other students or with previous years \cite{zhang2023learning}. This last paper reveals an increasing interest in FA combined with LA, with computer science courses leading the way, but with other disciplines and subjects also using it to improve success rates.
However, there are no general rules on what specific analytics should be used. In general, it is important to consider a learning (and possibly learner) model in order to be able to correctly measure learning outcomes and how they are reached. For instance, \cite{stanja2023formative} focuses on {\em conceptions}, how the students understand (or not) certain specific and common ideas, thus their process is geared towards using common assessment tools to measure understanding of those conceptions.  Student attitude and disposition towards learning can be as interesting as a model; however it is impossible to measure without explicitly asking the student; that is why authors such as \cite{tempelaar2020learning} propose a framework that combines traces with surveys in order to predict outcomes such as early drop out.

What seems to be missing is, however, a single set of learning process measurements that can be used across any platform, subject, or way of deployment of formative assessment strategies. This is what we will try to propose and evaluate next, after exposing how our class has been set up in the next Section.

\section{Class set up and methodology}
\label{sec:setup}

As indicated previously, we follow problem-based learning; this implies that the students need to work on their own project for the duration; they will do so along 10 objectives, numbered from 0 to 9. Every student will work on their own repository; for every objective they will create a branch and a pull request from that branch to their main one; a pull request is a "request to merge" a set of changes; acceptance of that PR will imply merging into the main branch (and, in real world, usually deployment to production). This class setup matches the agile philosophy which matches interaction and working software over "comprehensive documentation" (which, in a class environment, would mainly be "theoretical" classes and memory-based exams). At the same time, PRs are the main interaction medium in software development: it allows members of the team to perform quality (and other, such as compliance) checks on the code that is going to be used; this matches the requirements of formative evaluation too. The professor (and other students who have also overcome that specific objective) will review that PR, request changes if it falls short of reaching the objective, and eventually accept it when it does.

The student needs to pass objective number 5 to get a passing grade; every objective beyond that one works toward obtaining full marks. Objectives receive a different percentage of the total grade depending on the actual amount of work they need, not on the actual content. The percentage was adjusted depending on the amount of time students needed from submission to passing the objective in the first year. It did not need further adjustments in the following years. {\em Bigger} objectives get 15\% of the grade, while the smallest ones (for instance, the first one) can get only 5 \%. All students passing a certain objective obtain the same grade; this is why formative evaluation is often called {\em grade-less}, since learning objectives are reached or not, and when reached, everyone gets the same grade. Different grades correspond only to the fact that different objectives are reached.

Using GitHub PRs as a platform helps the creation of other workflows and work patterns around these PRs. Every time a PR submitted to the "central" repository passes tests, five random reviewers are drawn from the pool of other students that have already cleared that objective. Every objective, then, receives feedback not only from the teacher, but also from their peers. Again, this fits the agile mindset, but when students need to verbalize the issues with a PR by a colleague, it helps them settle the knowledge they have acquired in the objective they already passed. Additionally, it helps them understand the need for a efficient communication, through PRs, with other members of the team, and to use effectively these tools, closing the gap between classes and actual work\footnote{These reviews are made for additional credit; reaching all the objectives includes only 70\% of the final grade}.

Since these PRs happen in separate repositories (one for every student), we need a single, centralized place as a {\em platform} that helps to have an unified view of the whole class; this is carried out via a single repository, where the students submit a PR to a file that is different for each objective, and which triggers a workflow with a series of baseline tests of correctness of the PR submitted for clearing every objective. When the student overcomes the objective, a mark is inserted into the same file by the professor. Automatic workflows then analyze these files to generate a data file that records these events. This file is the single source of truth for the state of every objective for every student, and can be analyzed to study the progress of the class.

In general, we strive to make feedback as fast as possible, so that students can react to it in a timely way. The general rule is that we will react to requests for feedback (which are made either by a comment with mention in a pull request or using the specific mechanism GitHub has to request reviews) in less than 48h\footnote{Exception being some weekends, and official holidays}. A fast feedback is essential for the student to still have whatever conceptions or assumptions fresh so that they can be modified.

This implies a certain workload, mainly in the early stages when students submit their pull requests at pretty much the same time. However, early objectives require less effort to review than later ones; and when these later ones arrive, the teacher only has to chime in when one to three other students have already approved the submission. Although it is rather infrequent than no further suggestions have to been made, most frequent errors have already been ironed out by these reviews and request for changes. Additionally, a {\em frequent errors} specific for every objective is created, and this is specific for every school year. This is used by the students as a check list before submitting the review, but also, since it is created as submissions are reviewed, also by the teacher for quoting when these frequent errors show up in a pull request made by a student. In some cases specific errors that can be automatically checked show up (for instance, using capital letters in the names of files, something that is generally discouraged in repository-checked projects) they are incorporated into the automatic tests the PRs undergo when submitted to the central repository, so that they do not require additional rounds of reviewing by the teacher, releasing time needed for in-depth examination and feedback to the student.

In general, workload for the teacher is much more intensive than it is for {\em summative} evaluation or traditional {\em flipped learning} classes. However, the system has been set up so that most of the time spent by the teacher is effectively spent in giving constructive feedback to the student, so that they can reach their learning objectives.

During face to face classes, time is spent either addressing issues with specific students, or making small-group explanations to those that have just passed an objective. These whiteboard (never slides) expositions are adapted to specific circumstances of the student of group of students (early submitters, or late ones, for instance). In general, classes are working spaces where students and teacher interact with each other.

After the course is over, student GitHub nicks are anonymized with a one-way hash so that privacy is preserved\footnote{Even if students are warned to use whatever GitHub nick they want, including an anonymous, class-only one. The only thing needed for the teacher is to be able to clearly identify which student corresponds to every nick}. These are publicly available at \url{https://github.com/JJ/ivR}.

In Spain and for this subject, classes and evaluations are organized in this way:\begin{itemize}
\item A class period, which takes 15 weeks, for a total of 60 face-to-face hours. This is equivalent to 6 ECTS credits.
\item The {\em ordinary evaluation period}, which takes place up to 4 weeks later than the end of classes.
\item The {\em extraordinary evaluation period}, which takes place up to 3 weeks after the ordinary evaluation period.
\end{itemize}

The way the class is organized, whoever passes the minimum number of objectives during the ordinary period is evaluated during that period; if that is not the case, students can continue submitting their objectives up to the end of the extraordinary evaluation period, whatever objectives have been reached during the ordinary evaluation period do not need to be submitted.

Data has been collected in this way for three consecutive school years: 2021-22, 2022-23 and 2023-24\footnote{Incomplete at the time of this writing}. Additional data is collected in the form of surveys for specific events or activities, as well as at the beginning and at the end of every year\footnote{These surveys are validated every year and slightly changed according to student feedback. How specific surveys are validated will be mentioned when used}.

Next we will choose metrics and analyze the kind of information we can obtain from them, and how they can be used to improve learning outcomes.

\section{Results}
\label{sec:results}

A general description of the three courses over which we have been using this methodology are probably in order. We show general data about every course in Table \ref{tab:courses}.

\begin{table}
\caption{General data about the three courses\ref{tab:courses}}
\begin{tabular}{|l|c|c|c|}
\hline
Course & 2021-22 & 2022-23 & 2023-24\footnote{At the time of writing, these results are provisional} \\
Students & 51 & 48 & 40 \\
\% no-shows & 37.25\% & 66.66\% & 27\% \\
\% passed ordinary &  39.21\% & 20.83\% & 48\% \\
\% passed extraordinary & 23.53\% & 12.5\% & N/A \\
\% total passed & 62.74\% & 33.33\% & N/A \\
\hline

\end{tabular}
\end{table}

These statistics show that a methodology, by itself, is not enough to guarantee good results. The first year it was used the overall result was relatively good (more than 50\% passed), but 1/5th of the class needed additional time to pass. The next year, overall, was a disaster, with only 1/3rd of the class passing, and of those who did, more than 1/3rd needed to do so on additional time\footnote{Provisional results for this year are, however, quite promising, with the numbers of person who have passed already surpassing 21-22}. The causes of this are still being analyzed (and are beyond the scope of this paper), but what is clear is that specific changes need to be made from one year to the next so that whatever students fall in class every year are able to succeed. Please note that no-shows + passed add up to 100\%, since technically those that do not pass are not submitting their objectives, they are "not showing up" to be evaluated.

These end-of-course metrics will need real-time analytics to be able to intervene on class while there is still time to do it. And the metrics used in the learning analytics system should, ideally, be obtained directly from the student activity, without needing additional surveys. They should also be directly related to learning outcomes, and, if possible, learning attitude, possibly indirectly in this case. This is why in this case we have chosen two metrics:\begin{itemize}
\item The time in which a student submits their PR for an objective.
\item The time in which the teacher marks the objective as {\em passed} for the student.
\end{itemize}

In the first case, it is computed directly from the date the PR to the centralized repository is merged, that is, when the objective submitted clears the initial tests; a GitHub workflow computes it automatically and submits it to a data file that is also registered in the repository. It follows our requirements: it is obtained directly from the student activity, directly related to learning outcomes, namely, when the students feel they are ready at least to understand initial feedback from the teacher. Their attitude is reflected mainly in the time between clearing the previous objective and that submission, although that is clearly influenced by the difficulty of the objective itself.

In the second case, there might be a small delay between the moment the objective is accepted and the mark is done in the file and submitted to the repository, but this delay is usually a few minutes, and except for errors, less than one hour. The scale of this metric, or the difference between both times, is counted by days, so the small bias this might introduce is not really important.

In order to be able to normalize and compare different courses, these dates are counted from the day the course start. Data used in this paper is available at the same GitHub repository as this paper. Initial data as indicates is shown in Figure \ref{fig:initial}.
%
<<initial, echo=FALSE, warning=FALSE, fig.pos="h!tb", fig.env="figure*", fig.cap="Segment plot showing the number of days from the beginning of the course when objectives (indicated by color) have been submitted, with the segment end indicating when they have been approved. From bottom to top, this has been done for course 2021-22, 22-23 and 23-24 (still ongoing). The gap shows the period between the \"ordinary\" and \"extraordinary\" evaluation." >>=
library(ggplot2)
load("../data/entregas-data.rda")
# sort entregas.data by Curso and Entrega.Days
entregas.data <- entregas.data[order(entregas.data$Curso, entregas.data$Entrega.Days),]
entregas.data$idu <- 1:nrow(entregas.data)

ggplot(entregas.data, aes(x=Entrega.Days, y=idu, color=as.factor(Objetivo))) + scale_color_brewer(type="div") +
  geom_segment(data=entregas.data,aes(xend=Entrega.Days+Days.to.Superacion, yend=idu)) +
  geom_point(data=entregas.data,shape=0,aes(x=Entrega.Days, y=idu, color=as.factor(Objetivo))) +
  geom_point(data=entregas.data,shape=4,aes(x=Entrega.Days+Days.to.Superacion, y=idu, color=as.factor(Objetivo))) +
  xlim(0,160)+
  scale_y_discrete(limits=rev(levels(entregas.data$idu))) +
  labs(x="Days from the beginning", y="Order of submission", color="Objetivo") +
  theme_bw() +
  theme(legend.position="none")
@
%
This visualization already gives you an idea of the time needed to pass every objective: there is an abundance of long segments in the brown color, which is used for objectives number 2 and 4, the most complicated (thus receiving more credit). The slope that the proximal ends of the segments form also shows the rythm of submission, how it slows down during certain periods and then accelerates. But all this, besides giving you a general idea of how every course differs from the others, is not really a source of actionable intelligence, either from one year to the next or in real time for a specific course.

From this data we can try and extract some specific measure of individual performance, and the one that pops out most immediately is simply the difference between the time of submission and the time of approval, a measure of the effort the students put into learning the specific concepts and skills requires for every objective.

<<histogram.diff, echo=FALSE, warning=FALSE, fig.pos="h!tb", fig.cap="Histogram of the difference between the time of submission and the time of approval for every objective. Bins are 7 days wide">>=
ggplot(entregas.data[ entregas.data$Curso == "21-22",], aes(x=Days.to.Superacion,fill="21-22")) + geom_histogram(alpha=0.2,binwidth=7,position="identity") + geom_histogram(data=entregas.data[ entregas.data$Curso == "22-23",],aes(x=Days.to.Superacion,fill="22-23"), alpha=0.2,binwidth=7) + geom_histogram(data=entregas.data[ entregas.data$Curso == "23-24",],aes(x=Days.to.Superacion,fill="23-24"), alpha=0.2,binwidth=7) + labs(x="Days to approval", y="Number of objectives") + theme_bw()
@

Figure \ref{fig:histogram.diff} shows the histogram of this difference for every objective in every course; this already shows differences between the different courses, that go beyond the different number of students; the first two courses had approximately the same number of students (although, during 21-22, the number of {\em active} students, those that showed up at least one or submitted at least one objective, was much higher), while this year had less students. However, 21-22 and 23-24 show the same distribution, with the mode being student overcoming the objective in 1 week or less, while 22-23 shows that most students took between 1 and 2 weeks to pass every objective. This already shows the descriptive power of this metric: we already know that the results during 22-23 were not good, and this is correlated with the fact that they needed more time to pass every objective.

This metric, however, does not distinguish between students who passed and those who fell short of objective number 5, the minimum requirement. Will time-to-pass be a good predictor of success?

<<boxplot.pass, echo=FALSE, warning=FALSE, fig.pos="h!tb", fig.cap="Boxplot of the difference between the time of submission and the time of approval for every objective, grouped by whether the student passed or not.">>=
ggplot(entregas.data, aes(x=as.factor(Aprobado), y=Days.to.Superacion, fill=as.factor(Aprobado))) + geom_boxplot(notch = T) + labs(x="Pass", y="Days to approval") + theme_bw()
@
\bibliographystyle{ACM-Reference-Format}
\bibliography{../rpg.bib,../dev.bib, ../agile.bib,../learning-analytics.bib,../pbl.bib}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
